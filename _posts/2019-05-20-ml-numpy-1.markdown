---
title: "Ô∏èüí° Machine Learning in Python NumPy (Part 1): Neural Network in 9 Steps"
layout: post
date: 2019-05-20 23:00
published: true
headerImage: false
tag:
- DataScience
- MachineLearning
- DeepLearning
- Python
category: blog
author: edenau
description: "Understanding neural networks by coding"
# jemoji: '<img class="emoji" title=":open_file_folder:" alt=":open_file_folder:" src="https://assets.github.com/images/icons/emoji/unicode/1f5c2.png" height="20" width="20" align="absmiddle">'
---

***This post is now available on <a href="https://towardsdatascience.com/machine-learning-in-python-numpy-neural-network-in-9-steps-eafd0db25906" target="_blank">Towards Data Science ‚Äî Medium</a>. Check it out!***

***Source code is now available on <a href="https://github.com/edenau/ML-in-NumPy/blob/master/neural-net.ipynb" target="_blank">Github</a>. Check it out!***

# Motivation

If you are a **junior data scientist** who sort of understands how neural nets work, or a **machine learning enthusiast** who only knows a little about deep learning, this is the article that you cannot miss. Here is how you can build a neural net from scratch using NumPy in ***9 steps***‚Ää‚Äî‚Ääfrom data pre-processing to back-propagation‚Ää‚Äî‚Ääa must-do practice.

Basic understanding of machine learning, artificial neural network, Python syntax, and programming logic is preferred (but not necessary as you can learn on the go).

<div class="breaker"></div> <a id="1"></a>

# 1. Initialization

Step one. Import NumPy. Seriously.

{% gist b68b9803d38b691ab90680975b9bee24 %}

# 2. Data Generation

Deep learning is data-hungry. Although there are many clean datasets available online, we will generate our own for simplicity‚Ää‚Äî‚Ääfor inputs **a** and **b**, we have outputs **a+b**, **a-b**, and **\|a-b\|**. 10,000 datum points are generated.

{% gist d375786fd1452841e01820d2bdffae6c %}


# 3. Train-test Splitting

Our dataset is split into training (70%) and testing (30%) set. Only training set is leveraged for tuning neural networks. Testing set is used only for performance evaluation when the training is complete.

{% gist 95fa2ebc761b5a2ce3b5bc6389eec9d9 %}

# 4. Data Standardization

Data in the training set is standardized so that the distribution for each standardized feature is zero-mean and unit-variance. The scalers generated from the abovementioned procedure can then be applied to the testing set.

{% gist 81693265394ce40090d5bb037d9a009a %}

The scaler therefore does not contain any information from our testing set. We do not want our neural net to gain any information regarding testing set before network tuning.

We have now completed the data pre-processing procedures in ***4 steps***.

<div class="breaker"></div> <a id="2"></a>

# 5. Neural Net Construction

We objectify a ‚Äòlayer‚Äô using class in Python. Every layer (except the input layer) has a weight matrix **W**, a bias vector ***b***, and an activation function. Each layer is appended to a list called neural_net. That list would then be a representation of your fully connected neural network.

{% gist 8d338a1c4e631f5d80960fe695dd7ca4 %}

Finally, we do a sanity check on the number of hyperparameters using the following formula, and by counting. The number of datums available should exceed the number of hyperparameters, otherwise it will definitely overfit.

![1]({{ site.url }}/assets/posts/ml-numpy/nn-hp@2x.png)
N^l is number of hyperparameters at l-th layer, L is number of layers (excluding input layer)

# 6. Forward Propagation

We define a function for forward propagation given a certain set of weights and biases. The connection between layers is defined in matrix form as:

![2]({{ site.url }}/assets/posts/ml-numpy/forward_prop@2x.png)
œÉ is element-wise activation function, superscript T means transpose of a matrix

{% gist 00ad12e1bc45db76c3e67d0e9ab69d15 %}

Activation functions are defined one by one. ReLU is implemented as ***a ‚Üí max(a,0)***, whereas sigmoid function should return ***a ‚Üí 1/(1+e^(-a))***, and its implementation is left as an exercise to the reader.

# 7. Back-propagation

This is the most tricky part where many of us simply do not understand. Once we have defined a loss metric *e* for evaluating performance, we would like to know how the loss metric change when we perturb each weight or bias.

>We want to know how sensitive each weight and bias is with respect to the loss metric.

This is represented by partial derivatives **‚àÇe/‚àÇW** (denoted dW in code) and **‚àÇe/‚àÇb** (denoted db in code) respectively, and can be calculated analytically.

![3]({{ site.url }}/assets/posts/ml-numpy/dz@2x.png)
‚äô represents element-wise multiplication

![4]({{ site.url }}/assets/posts/ml-numpy/dW@2x.png)

![5]({{ site.url }}/assets/posts/ml-numpy/db@2x.png)

These back-propagation equations assume only one datum y is compared. The gradient update process would be very noisy as the performance of each iteration is subject to one datum point only. Multiple datums can be used to reduce the noise where **‚àÇW(y_1, y_2, ‚Ä¶)** would be the mean of **‚àÇW(y_1)**, **‚àÇW(y_2)**, **‚Ä¶**, and likewise for **‚àÇb**. This is not shown above in those equations, but is implemented in the code below.

{% gist 375003e850a2f53241547c75a8ce1a40 %}

<div class="breaker"></div> <a id="3"></a>

# 8. Iterative Optimization

We now have every building block for training a neural network.

Once we know the sensitivities of weights and biases, we try to ***minimize*** (hence the minus sign) the loss metric iteratively by gradient descent using the following update rule:

```
W = W - learning_rate * ‚àÇW
b = b - learning_rate * ‚àÇb
```

{% gist f2fedaee8478d073b972f218f91ffc81 %}
Training loss should be going down as it iterates

# 9. Testing

The model generalizes well if the testing loss is not much higher than the training loss. We also make some test cases to see how the model performs.

{% gist 6d92f4a4be3d4a7af6f36d6a4d19f0c3 %}


<div class="breaker"></div> <a id="4"></a>

# The Takeaway

This is how you can build a neural net from scratch using NumPy in ***9 steps***. Some of you might have already built neural nets using some high-level frameworks such as TensorFlow, PyTorch, or Keras. However, building a neural net using only low-level libraries enable us to truly understand the mathematics behind the mystery.

My implementation by no means is the most efficient way to build and train a neural net. There is so much room for improvement but that is a story for another day. Happy coding!
